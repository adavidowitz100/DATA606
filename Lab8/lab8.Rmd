---
title: "Data 606 - Lab 8 - Introduction to linear regression"
author: "Avery Davidowitz"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Import Libraries
```{r}
library(tidyverse)
library(openintro)
data('hfi', package='openintro')
```

## Exercise 1
What are the dimensions of the dataset?
```{r}
glimpse(hfi)
```

The data set is Rows: 1,458 by Columns: 123

## Exercise 2
What type of plot would you use to display the relationship between the personal freedom score, pf_score, and one of the other numerical variables? Plot this relationship using the variable pf_expression_control as the predictor. Does the relationship look linear? If you knew a country’s pf_expression_control, or its score out of 10, with 0 being the most, of political pressures and controls on media content, would you be comfortable using a linear model to predict the personal freedom score?



```{r}
ggplot2::ggplot(hfi, aes(x=pf_expression_control,y=pf_score)) + geom_point()
```
You would use a scatter plot to see a linear relationship between variables. There does appear to be a linear relationship in the data. I would be comfortable using a linear model to predict pf scores if other conditions such as normal residual requirements are also met. This is also supported by the relatively high correlation.

```{r}
hfi %>%
  summarise(cor(pf_expression_control, pf_score, use = "complete.obs"))
```
## Exercise 3
Looking at your plot from the previous exercise, describe the relationship between these two variables. Make sure to discuss the form, direction, and strength of the relationship as well as any unusual observations.

Given the previous plot you can see a relatively strong positive correlation between the variables. There is a slightly wider spread in the points closer to 0 which may indicate that a log transform would tighten the linearity (this effect is pretty weak). 

## Exercise 4
Using plot_ss, choose a line that does a good job of minimizing the sum of squares. Run the function several times. What was the smallest sum of squares that you got? How does it compare to your neighbors?

I run the functions and get:
Error in oSide[oSide < LLim | oSide > RLim] <- c(x + r)[oSide < LLim |  : 
  NAs are not allowed in subscripted assignments
  DATA606::plot_ss(x = pf_expression_control, y = pf_score, data = hfi, showSquares = TRUE)
Error in DATA606::plot_ss(x = pf_expression_control, y = pf_score, data = hfi,  : 
  unused argument (data = hfi)

## Exercise 5
Fit a new model that uses pf_expression_control to predict hf_score, or the total human freedom score. Using the estimates from the R output, write the equation of the regression line. What does the slope tell us in the context of the relationship between human freedom and the amount of political pressure on media content?
```{r}
m2 <- lm(hf_score ~ pf_expression_control, data = hfi)
summary(m2)
```

hf_score_hat = 5.153687 + .349862 * pf_expression_control
The slope tells us the line will have a slight positive rise. Meaning for every 1 units of pf_expression_control we expect hf_score to go up by about 1/3.

## Exercise 6
If someone saw the least squares regression line and not the actual data, how would they predict a country’s personal freedom school[sic] for one with a 6.7 rating for pf_expression_control? Is this an overestimate or an underestimate, and by how much? In other words, what is the residual for this prediction?

```{r}
m1 <- lm(pf_score ~ pf_expression_control, data = hfi)
summary(m1)
```

The prediction for 6.7 would lie on the regression line and produce a y value of:
```{r}
4.61707 + .49143 * 6.7
```
Knowing the data, we could compute the error or residual for the point for a pf_expression_control with value 6.7. However, there are no such values in our dataset.
```{r}
dplyr::filter(hfi,pf_expression_control==6.7)
```
The expected error associated with a prediction would be the Residual standard error: 0.8318 for each point predicted with the model. A reasonable assumption is that the actual value of a pf_score for a pf_expression_control==6.7 would be 7.909651 +/- 0.8318

## Exercise 7
Is there any apparent pattern in the residuals plot? What does this indicate about the linearity of the relationship between the two variables?

```{r}
ggplot(data = m1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")
```
The residuals seem pretty evenly distributed both about 0 and along the range of fitted values which indicates linearity and constant variability requirements for linear regression models are met.

## Exercise 8
Based on the histogram and the normal probability plot, does the nearly normal residuals condition appear to be met?

```{r}
ggplot(data = m1, aes(x = .resid)) +
  geom_histogram(binwidth = 1) +
  xlab("Residuals")
ggplot(data = m1, aes(sample = .resid)) +
  stat_qq()
```
The QQ plot indicates that the distribution of the residuals is nearly normal. The histogram also indicates a nearly normal distribution with no extreme outliers.

## Exercise 9
Based on the residuals vs. fitted plot, does the constant variability condition appear to be met?

Yes. Based on the plot the constant variability condition appears to be met.


## More Practice
1.Choose another freedom variable and a variable you think would strongly correlate with it.. Produce a scatterplot of the two variables and fit a linear model. At a glance, does there seem to be a linear relationship?
```{r}
ggplot2::ggplot(hfi, aes(x=ef_legal,y=ef_score)) + geom_point()
hfi %>%
  summarise(cor(ef_legal, ef_score, use = "complete.obs"))
```
There does appear to be a strong linear relationship between the variables ef_legal and overall ef_score. The correlation also supports the linearity.

2.How does this relationship compare to the relationship between pf_expression_control and pf_score? Use the R2 values from the two model summaries to compare. Does your independent variable seem to predict your dependent one better? Why or why not?

```{r}
m3 <- lm(ef_score ~ ef_legal, data = hfi)
summary(m3)
ggplot(data = m3, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")
ggplot(data = m3, aes(x = .resid)) +
  geom_histogram(binwidth = .5) +
  xlab("Residuals")
ggplot(data = m3, aes(sample = .resid)) +
  stat_qq()
```

Both of these models perform similarly with R squared values of .634 for m1 compared to .61 for this model.

3.What’s one freedom relationship you were most surprised about and why? Display the model diagnostics for the regression model analyzing this relationship.

```{r}
m4 <- lm(pf_score ~ ef_score, data = hfi)
summary(m4)
```

I'm surprised that the relationship between overall economic freedom and personal freedom is not more highly associated.